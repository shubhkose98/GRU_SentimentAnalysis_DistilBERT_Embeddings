# Sentiment Classification using GRU and Bidirectional GRU Networks with DistilBERT Embeddings

## Overview
This project involves Implementation of RNN using GRU and Bidirectional GRU Networks for Sentiment Classification with word embeddings generated by the DistilBERT transformer model. The embeddings were used as input to two different GRU-based models: a Unidirectional GRU (GRUnet) and a Bidirectional GRU (BiGRUnet). Both models were trained to predict the sentiment of the review based on these embeddings.

## Model Implementation
### Unidirectional GRU (GRUnet)
- A Gated Recurrent Unit (GRU) is a type of recurrent neural network (RNN) architecture designed to address the vanishing gradient problem often encountered in traditional RNNs. GRU models are characterized by their gating mechanisms, which regulate the flow of information within the network.
 - A unidirectional GRU with an input size of 768 (DistilBERT embedding size), hidden size of 100, and an output size of 3.
 - The GRU output is passed through a fully connected layer followed by ReLU and LogSoftmax activations for classification.
### Bidirectional GRU (BiGRUnet)
- A Bidirectional Gated Recurrent Unit (GRU) model is an extension of the standard GRU architecture that enables the network to process input sequences in both forward and backward directions. In bidirectional GRU models, the input sequence is fed into two separate GRU layers: one processing the sequence in the forward direction and the other in the backward direction
 - A bidirectional GRU that processes sequences in both directions to capture past and future contexts, allowing the model to capture dependencies from both directions.
 - Similar to GRUnet, the output is passed through a fully connected layer with ReLU and LogSoftmax to produce the final probabilities.

## Dataset Prepration
The implementation of the sentiment analysis was performed on two datasets:
- Financial Sentiment dataset used in the project is provided in the repo by the name "data.csv".
- Amazon User Feedback review dataset is provided in this [link](https://engineering.purdue.edu/kak/distDLS/text_datasets_for_DLStudio.tar.gz)
For the processed word embeddings to be passed into the PyTorch dataloader we use the DistilBert model from the Transformers library provided by huggingface. The detailed implemenation for both the datasets can be found in the attached documentation.

## Training
- Both GRUnet and BiGRUnet were trained for 10 epochs with a batch size of 1.
- Learning Rate: 1e-4, Optimizer: Adam (betas = (0.8, 0.999)).
 - The training process involved alternating between updating the GRU weights and calculating the loss using Cross-Entropy Loss.

## Notes
- Implementation details, required libraries and performance evaluation of the models can be found in the attached PDF.
- Please refer to the attached PDF file for further details on the outputs' implementation and visual representation and details of all the libraries and datasets required. Also, please check the imports in the .ipynb or .py file.
